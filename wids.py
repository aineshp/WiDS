# -*- coding: utf-8 -*-
"""wids.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OOtfs0UuXeUj_uOqln-LlONZ6fMi1I2p
"""



!git clone https://github.com/MrigankGoel/Tokens-to-Thought-A-Contextual-Transformer/

# Commented out IPython magic to ensure Python compatibility.
# %cd Tokens-to-Thought-A-Contextual-Transformer
!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd Week4
!ls

# =========================
# Imports
# =========================
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, Input, Embedding, LayerNormalization, Dropout
from tensorflow.keras.models import Model
import numpy as np

# =========================
# Load text
# =========================
def load_text(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        data = f.read().replace("\n", " ")
    return data

data = load_text("training_data.txt")
print("Total characters:", len(data))

# =========================
# Vocabulary
# =========================
def build_vocab(data: str):
    characters = list(set(data))
    vocab_size = len(characters) + 1

    char2idx = {ch: i + 1 for i, ch in enumerate(characters)}
    idx2char = {i + 1: ch for i, ch in enumerate(characters)}

    return characters, vocab_size, char2idx, idx2char

characters, vocab_size, char2idx, idx2char = build_vocab(data)
print("Vocab size:", vocab_size)

# =========================
# Encode / Decode
# =========================
def encode(text):
    return [char2idx[c] for c in text]

def decode(tokens):
    return "".join(idx2char[t] for t in tokens if t in idx2char)

encoded = encode(data)

# =========================
# Train / Test Split
# =========================
split = int(0.9 * len(encoded))
train_data = encoded[:split]
test_data  = encoded[split:]

# =========================
# Hyperparameters
# =========================
batch_size = 32
block_size = 128
num_heads = 8
num_transformer_blocks = 4
embed_dim = 256
feed_forward_dim = 256
dropout_rate = 0.1

# =========================
# Causal Attention Mask
# =========================
def causal_attention_mask(batch_size, n_dest, n_src):
    i = tf.range(n_dest)[:, None]
    j = tf.range(n_src)[None, :]
    mask = i >= j
    mask = tf.reshape(mask, (1, n_dest, n_src))
    return tf.tile(mask, [batch_size, 1, 1])

# =========================
# Causal Attention Mask
# =========================
def causal_attention_mask(batch_size, n_dest, n_src):
    i = tf.range(n_dest)[:, None]
    j = tf.range(n_src)[None, :]
    mask = i >= j
    mask = tf.reshape(mask, (1, n_dest, n_src))
    return tf.tile(mask, [batch_size, 1, 1])

# =========================
# Transformer Block
# =========================
class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()

        self.att = layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=embed_dim // num_heads
        )

        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])

        self.norm1 = LayerNormalization(epsilon=1e-6)
        self.norm2 = LayerNormalization(epsilon=1e-6)
        self.drop1 = Dropout(rate)
        self.drop2 = Dropout(rate)

    def call(self, x, training=False):
        batch_size = tf.shape(x)[0]
        seq_len = tf.shape(x)[1]

        mask = causal_attention_mask(batch_size, seq_len, seq_len)

        attn = self.att(x, x, attention_mask=mask)
        attn = self.drop1(attn, training=training)
        out1 = self.norm1(x + attn)

        ffn = self.ffn(out1)
        ffn = self.drop2(ffn, training=training)
        return self.norm2(out1 + ffn)

# =========================
# Token + Position Embedding
# =========================
class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super().__init__()
        self.token_emb = Embedding(vocab_size, embed_dim)
        self.pos_emb = Embedding(maxlen, embed_dim)

    def call(self, x):
        seq_len = tf.shape(x)[1]
        positions = tf.range(0, seq_len)
        return self.token_emb(x) + self.pos_emb(positions)

# =========================
# Model Builder
# =========================
def get_transformer_model():
    inputs = Input(shape=(block_size,), dtype=tf.int32)
    x = TokenAndPositionEmbedding(block_size, vocab_size, embed_dim)(inputs)

    for _ in range(num_transformer_blocks):
        x = TransformerBlock(embed_dim, num_heads, feed_forward_dim, dropout_rate)(x)

    outputs = Dense(vocab_size)(x)
    return Model(inputs, outputs)

# =========================
# Compile Model
# =========================
model = get_transformer_model()

model.compile(
    optimizer="adam",
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"]
)

model.summary()

# =========================
# Dataset Preparation
# =========================
def make_dataset(data, block_size):
    X, Y = [], []
    for i in range(len(data) - block_size):
        X.append(data[i:i+block_size])
        Y.append(data[i+1:i+block_size+1])
    return np.array(X), np.array(Y)

X_train, Y_train = make_dataset(train_data, block_size)

dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))
dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)

# =========================
# Train
# =========================
history = model.fit(dataset, epochs=10)

# =========================
# Text Generation
# =========================
def generate_text(model, seed_tokens, num_generate=300):
    tokens = list(seed_tokens)

    for _ in range(num_generate):
        x = tf.convert_to_tensor([tokens[-block_size:]], dtype=tf.int32)
        logits = model(x)
        logits = logits[0, -1]
        probs = tf.nn.softmax(logits)
        next_token = tf.random.categorical(tf.math.log([probs]), 1)[0, 0].numpy()
        tokens.append(next_token)

    return decode(tokens)

# =========================
# Sample Output
# =========================
seed = train_data[:block_size]
print(generate_text(model, seed))